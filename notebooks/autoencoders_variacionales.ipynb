{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders variacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autoencoder variacionales es un tipo de modelo generativo (basado en DNN, que son un tipo de función anidada) que modela la distribución de probabilidad de cierto conjunto de datos. \n",
    "\n",
    "La formula general de los modelos generativos es:\n",
    "\n",
    "$P(x; \\theta) $\n",
    "\n",
    "Donde $x$ es el dato que se quiere modelar y $\\theta$ son los parámetros del modelo.\n",
    "\n",
    "Para estimar esta ditribución de probabilidad se utiliza la funcion de maxima verosimilitud (argmax del log  (P(x; \\theta)):\n",
    "\n",
    "$argmax_{\\theta} log(P(x; \\theta))$\n",
    "\n",
    "Se emplea la función de logaritmo por sus  cualidades matemáticas. Cuando se tiene una cantidad grande de datos, el calculo de la verosimilitud de un conjunto de datos es el producto de las verosimilitudes de cada uno, lo cual puede llevar a problemas de computo (inestabilidad numérica, problemas de redondeo, etc). Al emplear la función logaritmo, se convierte el producto en una suma, facilitando las operaciones. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El desafío que encontramos en el algoritmo de maximización de la verosimilitud aparece cuando encontramos datos no observados, es decir, cuando no tenemos la información completa de los datos. En este caso, la verosimilitud se convierte en un logaritmo de una integral o suma de todas las probabilidades condicionales de los datos no observados (es condicional pues se condicional la probabilidad de los datos no observados a los datos observados).\n",
    "\n",
    "Aquí surge la alternativa de ELBO (Evidence Lower Bound), que es una cota inferior de la verosimilitud. La idea es maximizar ELBO en lugar de la verosimilitud. Esta cota inferior se obtiene a partir de la divergencia de Kullback-Leibler (KL) entre la distribución de probabilidad real y la distribución de probabilidad que se está modelando. \n",
    "\n",
    "Es una cota inferior de la verosimilitud, es decir que la verosimilitud es mayor o igual a ELBO. Dado que la función logarítmica es creciente y cóncava, podemos aplicar la desigualdad de Jensen para obtener una cota inferior de la verosimilitud. Esta cota inferior es conocida como la Evidence Lower Bound (ELBO) y se utiliza en técnicas de inferencia variacional para aproximar distribuciones posteriores complejas. La maximización de ELBO, en lugar de la verosimilitud exacta, permite realizar inferencias y aprender parámetros de modelos probabilísticos de manera eficiente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
