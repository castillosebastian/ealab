{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Description\n",
    "\n",
    "## Machine Learning Algorithms Summary\n",
    "\n",
    "### 1. **Linear Models**:\n",
    "- **LDA (Linear Discriminant Analysis)**: Utilizes linear decision surfaces for class differentiation, focusing on maximizing class mean differences and minimizing within-class variance.\n",
    "- **Ridge Classifier**: Implements L2 regularization to address multicollinearity, enhancing model generalization.\n",
    "- **SGD (Stochastic Gradient Descent) Classifier**: Employs stochastic gradient descent for optimizing linear models under convex loss functions.\n",
    "\n",
    "### 2. **Tree-Based Models**:\n",
    "- **Decision Trees (DTC, DecisionTreeClassifier; ETC, ExtraTreeClassifier)**: Splits data into branches for decision-making, based on feature values.\n",
    "- **Random Forest (RandomForestClassifier)**: An ensemble of decision trees that combines predictions for better generalization.\n",
    "- **Extra Trees Ensemble (ExtraTreesClassifier)**: Similar to Random Forests but with increased model variance due to randomization in decisions.\n",
    "- **Gradient Boosting (GradientBoostingClassifier)**: Builds trees sequentially to correct previous errors, enhancing predictive performance.\n",
    "\n",
    "### 3. **Ensemble Methods**:\n",
    "- **AdaBoost (AdaBoostClassifier)**: Improves decision trees by adjusting to misclassifications in training iterations.\n",
    "- **Bagging (BaggingClassifier)**: Enhances stability and accuracy by averaging multiple model predictions.\n",
    "\n",
    "### 4. **Support Vector Machines**:\n",
    "- **LSVC (LinearSVC)**: A linear implementation of Support Vector Classification.\n",
    "- **NuSVC (NuSVC)**: Uses a parameter to control support vector quantity, similar to SVC.\n",
    "- **SVC (Support Vector Classification)**: Effective in high-dimensional spaces, relies on support vectors for decision boundaries.\n",
    "\n",
    "### 5. **Naive Bayes**:\n",
    "- **BNB (BernoulliNB)**: Optimized for binary/boolean feature datasets.\n",
    "- **GNB (GaussianNB)**: Assumes Gaussian distribution of features.\n",
    "\n",
    "### 6. **Nearest Neighbor**:\n",
    "- **KNN (KNeighborsClassifier)**: Classifies based on the majority class of k-nearest neighbors.\n",
    "\n",
    "### 7. **Neural Networks**:\n",
    "- **MLP (MLPClassifier)**: Multilayer Perceptron, suitable for complex datasets with large feature sets.\n",
    "\n",
    "### 8. **Discriminant Analysis**:\n",
    "- **QDA (Quadratic Discriminant Analysis)**: Allows non-linear class separation, an extension of LDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) by scikit-lear \n",
    "\n",
    "SGD is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n",
    "\n",
    "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.\n",
    "\n",
    "Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. Often, an instance of SGDClassifier or SGDRegressor will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using SGDClassifier(loss='log_loss') results in logistic regression, i.e. a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression. Similarly, SGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means.\n",
    "\n",
    "The advantages of Stochastic Gradient Descent are:\n",
    "\n",
    "Efficiency.\n",
    "\n",
    "Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "The disadvantages of Stochastic Gradient Descent include:\n",
    "\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/sgd.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desempeño de los modelos ML en los dataset seleccionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Madelon**: los modelos de base lineal (LDA, LSVC, Ridge) y bayesianos (BNB y GNB) tuvieron un bajo desempeño. Buen desempeño presentaron los modelos basados en arboles de decisión y modelos boosteados, como también MLP. Aquellos algoritmos que pueden modelar funciones objetivo más complejas tuvieron mejor resultado."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
